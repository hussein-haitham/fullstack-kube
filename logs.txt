
==> Audit <==
|---------|------------------------------------------|----------|--------------|---------|----------------------|----------------------|
| Command |                   Args                   | Profile  |     User     | Version |      Start Time      |       End Time       |
|---------|------------------------------------------|----------|--------------|---------|----------------------|----------------------|
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 03:37 CEST |                      |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 03:41 CEST |                      |
| start   | --force                                  | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 03:42 CEST |                      |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:32 CEST |                      |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:33 CEST |                      |
| delete  |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:37 CEST | 09 May 25 21:37 CEST |
| start   | --driver=docker --force                  | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:37 CEST | 09 May 25 21:38 CEST |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:44 CEST | 09 May 25 21:44 CEST |
| image   | load mongo.tar                           | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:44 CEST | 09 May 25 21:45 CEST |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:45 CEST | 09 May 25 21:45 CEST |
| image   | load app-frontend-mongo.tar              | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:45 CEST | 09 May 25 21:45 CEST |
| image   | load app-frontend-mongo.tar              | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:47 CEST | 09 May 25 21:47 CEST |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:48 CEST | 09 May 25 21:48 CEST |
| image   | load app-backend-mongo.tar               | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:49 CEST | 09 May 25 21:49 CEST |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 21:49 CEST | 09 May 25 21:49 CEST |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:12 CEST | 09 May 25 22:12 CEST |
| service | app-frontend-service                     | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:16 CEST |                      |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:23 CEST | 09 May 25 22:23 CEST |
| image   | load                                     | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:31 CEST | 09 May 25 22:32 CEST |
|         | husseinhaitham/app-frontend-mongo:latest |          |              |         |                      |                      |
| image   | ls                                       | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:32 CEST | 09 May 25 22:32 CEST |
| ip      |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:34 CEST | 09 May 25 22:34 CEST |
| stop    |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:36 CEST | 09 May 25 22:36 CEST |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:36 CEST | 09 May 25 22:37 CEST |
| ip      |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:41 CEST | 09 May 25 22:41 CEST |
| service | app-frontend-service --url               | minikube | KNOWISAG\HHa | v1.35.0 | 09 May 25 22:47 CEST | 09 May 25 22:48 CEST |
| service | app-frontend --url                       | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:21 CEST |                      |
| service | app-frontend-service --url               | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:22 CEST | 11 May 25 18:23 CEST |
| service | app-backend-service --url                | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:23 CEST | 11 May 25 18:24 CEST |
| service | app-frontend-service                     | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:52 CEST | 11 May 25 18:53 CEST |
| ip      |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:57 CEST | 11 May 25 18:57 CEST |
| service | app-frontend-service                     | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 18:59 CEST |                      |
| stop    |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 19:23 CEST | 11 May 25 19:23 CEST |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 19:23 CEST | 11 May 25 19:24 CEST |
| service | app-frontend-service                     | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 19:27 CEST |                      |
| service | app-frontend-service                     | minikube | KNOWISAG\HHa | v1.35.0 | 11 May 25 19:28 CEST |                      |
| start   |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 15:23 CEST | 03 Jun 25 15:24 CEST |
| addons  | enable ingress                           | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 15:42 CEST |                      |
| addons  | disable ingress                          | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 15:56 CEST | 03 Jun 25 15:56 CEST |
| addons  | enable ingress                           | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 15:56 CEST |                      |
| stop    |                                          | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 16:03 CEST | 03 Jun 25 16:03 CEST |
| start   | --memory=4096 --cpus=2                   | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 16:04 CEST | 03 Jun 25 16:05 CEST |
| addons  | enable ingress                           | minikube | KNOWISAG\HHa | v1.35.0 | 03 Jun 25 16:06 CEST |                      |
|---------|------------------------------------------|----------|--------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/06/03 16:04:06
Running on machine: c-rnb-071
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0603 16:04:06.624410   25284 out.go:345] Setting OutFile to fd 484 ...
I0603 16:04:06.636069   25284 out.go:397] isatty.IsTerminal(484) = true
I0603 16:04:06.636069   25284 out.go:358] Setting ErrFile to fd 484...
I0603 16:04:06.636069   25284 out.go:397] isatty.IsTerminal(484) = true
I0603 16:04:09.417314   25284 out.go:352] Setting JSON to false
I0603 16:04:09.426926   25284 start.go:129] hostinfo: {"hostname":"c-rnb-071","uptime":13185,"bootTime":1748946263,"procs":361,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.5335 Build 22631.5335","kernelVersion":"10.0.22631.5335 Build 22631.5335","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"546bc762-46f7-441c-87f3-7fa3b48e4b22"}
W0603 16:04:09.426926   25284 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0603 16:04:09.429651   25284 out.go:177] 😄  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.22631.5335 Build 22631.5335
I0603 16:04:09.430721   25284 notify.go:220] Checking for updates...
I0603 16:04:09.433387   25284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0603 16:04:09.433387   25284 driver.go:394] Setting default libvirt URI to qemu:///system
I0603 16:04:09.749152   25284 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0603 16:04:09.778297   25284 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0603 16:04:10.341111   25284 info.go:266] docker info: {ID:761a6c19-438b-475c-9ed9-3b4316cf5428 Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:93 SystemTime:2025-06-03 14:04:10.311966524 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:16599330816 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0603 16:04:10.342850   25284 out.go:177] ✨  Using the docker driver based on existing profile
I0603 16:04:10.343879   25284 start.go:297] selected driver: docker
I0603 16:04:10.343879   25284 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HHa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0603 16:04:10.343879   25284 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0603 16:04:10.565473   25284 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0603 16:04:11.064533   25284 info.go:266] docker info: {ID:761a6c19-438b-475c-9ed9-3b4316cf5428 Containers:11 ContainersRunning:0 ContainersPaused:0 ContainersStopped:11 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:93 SystemTime:2025-06-03 14:04:11.047444098 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:16599330816 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
W0603 16:04:11.065088   25284 out.go:270] ❗  You cannot change the memory size for an existing minikube cluster. Please first delete the cluster.
I0603 16:04:11.065088   25284 cni.go:84] Creating CNI manager for ""
I0603 16:04:11.065088   25284 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0603 16:04:11.065641   25284 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HHa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0603 16:04:11.067516   25284 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0603 16:04:11.068724   25284 cache.go:121] Beginning downloading kic base image for docker with docker
I0603 16:04:11.069387   25284 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0603 16:04:11.070669   25284 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0603 16:04:11.070669   25284 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0603 16:04:11.071318   25284 preload.go:146] Found local preload: C:\Users\HHa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0603 16:04:11.071318   25284 cache.go:56] Caching tarball of preloaded images
I0603 16:04:11.071958   25284 preload.go:172] Found C:\Users\HHa\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0603 16:04:11.071958   25284 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0603 16:04:11.072917   25284 profile.go:143] Saving config to C:\Users\HHa\.minikube\profiles\minikube\config.json ...
I0603 16:04:11.361910   25284 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0603 16:04:11.361910   25284 localpath.go:146] windows sanitize: C:\Users\HHa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\HHa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0603 16:04:11.362435   25284 localpath.go:146] windows sanitize: C:\Users\HHa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\HHa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0603 16:04:11.362435   25284 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0603 16:04:11.362435   25284 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0603 16:04:11.362435   25284 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0603 16:04:11.362435   25284 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0603 16:04:11.362435   25284 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0603 16:04:11.362435   25284 localpath.go:146] windows sanitize: C:\Users\HHa\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\HHa\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0603 16:04:41.575466   25284 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0603 16:04:41.576130   25284 cache.go:227] Successfully downloaded all kic artifacts
I0603 16:04:41.577477   25284 start.go:360] acquireMachinesLock for minikube: {Name:mk62910b1ba561515d5efd9005c76110b9ad604a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0603 16:04:41.577477   25284 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0603 16:04:41.578074   25284 start.go:96] Skipping create...Using existing machine configuration
I0603 16:04:41.578074   25284 fix.go:54] fixHost starting: 
I0603 16:04:41.756599   25284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0603 16:04:41.881080   25284 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0603 16:04:41.881923   25284 fix.go:138] unexpected machine state, will restart: <nil>
I0603 16:04:41.882596   25284 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0603 16:04:41.918951   25284 cli_runner.go:164] Run: docker start minikube
I0603 16:04:42.491853   25284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0603 16:04:42.670177   25284 kic.go:430] container "minikube" state is running.
I0603 16:04:42.755791   25284 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0603 16:04:42.927714   25284 profile.go:143] Saving config to C:\Users\HHa\.minikube\profiles\minikube\config.json ...
I0603 16:04:42.936364   25284 machine.go:93] provisionDockerMachine start ...
I0603 16:04:43.073096   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:43.300919   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:43.304343   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:43.304343   25284 main.go:141] libmachine: About to run SSH command:
hostname
I0603 16:04:43.648789   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0603 16:04:43.648789   25284 ubuntu.go:169] provisioning hostname "minikube"
I0603 16:04:43.805852   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:43.984912   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:43.988049   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:43.988049   25284 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0603 16:04:44.455767   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0603 16:04:44.518531   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:44.677489   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:44.679676   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:44.679807   25284 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0603 16:04:44.858855   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0603 16:04:44.858855   25284 ubuntu.go:175] set auth options {CertDir:C:\Users\HHa\.minikube CaCertPath:C:\Users\HHa\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\HHa\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\HHa\.minikube\machines\server.pem ServerKeyPath:C:\Users\HHa\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\HHa\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\HHa\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\HHa\.minikube}
I0603 16:04:44.858855   25284 ubuntu.go:177] setting up certificates
I0603 16:04:44.858855   25284 provision.go:84] configureAuth start
I0603 16:04:44.904465   25284 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0603 16:04:45.034819   25284 provision.go:143] copyHostCerts
I0603 16:04:45.037008   25284 exec_runner.go:144] found C:\Users\HHa\.minikube/ca.pem, removing ...
I0603 16:04:45.037008   25284 exec_runner.go:203] rm: C:\Users\HHa\.minikube\ca.pem
I0603 16:04:45.037702   25284 exec_runner.go:151] cp: C:\Users\HHa\.minikube\certs\ca.pem --> C:\Users\HHa\.minikube/ca.pem (1070 bytes)
I0603 16:04:45.040409   25284 exec_runner.go:144] found C:\Users\HHa\.minikube/cert.pem, removing ...
I0603 16:04:45.040409   25284 exec_runner.go:203] rm: C:\Users\HHa\.minikube\cert.pem
I0603 16:04:45.040409   25284 exec_runner.go:151] cp: C:\Users\HHa\.minikube\certs\cert.pem --> C:\Users\HHa\.minikube/cert.pem (1115 bytes)
I0603 16:04:45.043138   25284 exec_runner.go:144] found C:\Users\HHa\.minikube/key.pem, removing ...
I0603 16:04:45.043138   25284 exec_runner.go:203] rm: C:\Users\HHa\.minikube\key.pem
I0603 16:04:45.043645   25284 exec_runner.go:151] cp: C:\Users\HHa\.minikube\certs\key.pem --> C:\Users\HHa\.minikube/key.pem (1675 bytes)
I0603 16:04:45.045268   25284 provision.go:117] generating server cert: C:\Users\HHa\.minikube\machines\server.pem ca-key=C:\Users\HHa\.minikube\certs\ca.pem private-key=C:\Users\HHa\.minikube\certs\ca-key.pem org=HHa.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0603 16:04:45.579337   25284 provision.go:177] copyRemoteCerts
I0603 16:04:45.655019   25284 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0603 16:04:45.693571   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:45.819823   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:04:45.949689   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0603 16:04:46.000750   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0603 16:04:46.044146   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0603 16:04:46.078620   25284 provision.go:87] duration metric: took 1.2197778s to configureAuth
I0603 16:04:46.078620   25284 ubuntu.go:193] setting minikube options for container-runtime
I0603 16:04:46.080357   25284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0603 16:04:46.119827   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:46.246757   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:46.248026   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:46.248026   25284 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0603 16:04:46.410301   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0603 16:04:46.410301   25284 ubuntu.go:71] root file system type: overlay
I0603 16:04:46.410811   25284 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0603 16:04:46.457047   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:46.613883   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:46.615559   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:46.615559   25284 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0603 16:04:46.816355   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0603 16:04:46.870234   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:47.009227   25284 main.go:141] libmachine: Using SSH client type: native
I0603 16:04:47.010370   25284 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd95360] 0xd97ea0 <nil>  [] 0s} 127.0.0.1 59420 <nil> <nil>}
I0603 16:04:47.010370   25284 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0603 16:04:47.228466   25284 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0603 16:04:47.228466   25284 machine.go:96] duration metric: took 4.292147s to provisionDockerMachine
I0603 16:04:47.228466   25284 start.go:293] postStartSetup for "minikube" (driver="docker")
I0603 16:04:47.228466   25284 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0603 16:04:47.280053   25284 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0603 16:04:47.312509   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:47.437094   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:04:47.571099   25284 ssh_runner.go:195] Run: cat /etc/os-release
I0603 16:04:47.578745   25284 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0603 16:04:47.578745   25284 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0603 16:04:47.578745   25284 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0603 16:04:47.578745   25284 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0603 16:04:47.578745   25284 filesync.go:126] Scanning C:\Users\HHa\.minikube\addons for local assets ...
I0603 16:04:47.579487   25284 filesync.go:126] Scanning C:\Users\HHa\.minikube\files for local assets ...
I0603 16:04:47.580101   25284 start.go:296] duration metric: took 351.6382ms for postStartSetup
I0603 16:04:47.586544   25284 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0603 16:04:47.624489   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:47.741816   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:04:47.887885   25284 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0603 16:04:47.897659   25284 fix.go:56] duration metric: took 6.3196509s for fixHost
I0603 16:04:47.897659   25284 start.go:83] releasing machines lock for "minikube", held for 6.3202475s
I0603 16:04:47.940903   25284 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0603 16:04:48.068026   25284 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0603 16:04:48.070896   25284 ssh_runner.go:195] Run: cat /version.json
I0603 16:04:48.107372   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:48.109084   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:04:48.244005   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:04:48.256692   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
W0603 16:04:48.377156   25284 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0603 16:04:48.444767   25284 ssh_runner.go:195] Run: systemctl --version
I0603 16:04:48.466704   25284 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0603 16:04:48.554968   25284 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0603 16:04:48.573131   25284 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0603 16:04:48.646371   25284 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0603 16:04:48.677356   25284 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0603 16:04:48.677356   25284 start.go:495] detecting cgroup driver to use...
I0603 16:04:48.677356   25284 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0603 16:04:48.677939   25284 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0603 16:04:48.725345   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0603 16:04:48.754457   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0603 16:04:48.772842   25284 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0603 16:04:48.782127   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0603 16:04:48.812602   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0603 16:04:48.841190   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0603 16:04:48.867149   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0603 16:04:48.875566   25284 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0603 16:04:48.875566   25284 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0603 16:04:48.899696   25284 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0603 16:04:48.944179   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0603 16:04:48.978244   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0603 16:04:49.008032   25284 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0603 16:04:49.098000   25284 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0603 16:04:49.174002   25284 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0603 16:04:49.247675   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:04:49.473078   25284 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0603 16:04:49.721657   25284 start.go:495] detecting cgroup driver to use...
I0603 16:04:49.721657   25284 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0603 16:04:49.799383   25284 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0603 16:04:49.813393   25284 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0603 16:04:49.895675   25284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0603 16:04:49.909868   25284 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0603 16:04:50.074019   25284 ssh_runner.go:195] Run: which cri-dockerd
I0603 16:04:50.191867   25284 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0603 16:04:50.206605   25284 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0603 16:04:50.302823   25284 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0603 16:04:50.523031   25284 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0603 16:04:50.661247   25284 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0603 16:04:50.661806   25284 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0603 16:04:50.767381   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:04:50.984713   25284 ssh_runner.go:195] Run: sudo systemctl restart docker
I0603 16:04:55.488435   25284 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.5035626s)
I0603 16:04:55.586891   25284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0603 16:04:55.741374   25284 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0603 16:04:55.868816   25284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0603 16:04:55.978983   25284 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0603 16:04:56.241642   25284 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0603 16:04:56.479898   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:04:56.761614   25284 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0603 16:04:56.915113   25284 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0603 16:04:57.033497   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:04:57.299179   25284 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0603 16:04:57.474518   25284 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0603 16:04:57.493199   25284 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0603 16:04:57.508634   25284 start.go:563] Will wait 60s for crictl version
I0603 16:04:57.530303   25284 ssh_runner.go:195] Run: which crictl
I0603 16:04:57.622156   25284 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0603 16:04:57.740473   25284 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0603 16:04:57.768800   25284 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0603 16:04:57.962026   25284 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0603 16:04:58.021558   25284 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0603 16:04:58.073648   25284 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0603 16:04:58.524136   25284 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0603 16:04:58.532941   25284 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0603 16:04:58.546021   25284 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0603 16:04:58.671455   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0603 16:04:58.846354   25284 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HHa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0603 16:04:58.846354   25284 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0603 16:04:58.890359   25284 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0603 16:04:58.931251   25284 docker.go:689] Got preloaded images: -- stdout --
husseinhaitham/app-frontend-mongo:latest
husseinhaitham/app-backend-mongo:latest
mongo:8.0
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0603 16:04:58.931251   25284 docker.go:619] Images already preloaded, skipping extraction
I0603 16:04:58.994284   25284 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0603 16:04:59.036268   25284 docker.go:689] Got preloaded images: -- stdout --
husseinhaitham/app-frontend-mongo:latest
husseinhaitham/app-backend-mongo:latest
mongo:8.0
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0603 16:04:59.036268   25284 cache_images.go:84] Images are preloaded, skipping loading
I0603 16:04:59.036268   25284 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0603 16:04:59.036268   25284 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0603 16:04:59.149764   25284 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0603 16:04:59.255006   25284 cni.go:84] Creating CNI manager for ""
I0603 16:04:59.255006   25284 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0603 16:04:59.255006   25284 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0603 16:04:59.255006   25284 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0603 16:04:59.255006   25284 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0603 16:04:59.316114   25284 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0603 16:04:59.328413   25284 binaries.go:44] Found k8s binaries, skipping transfer
I0603 16:04:59.388345   25284 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0603 16:04:59.403591   25284 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0603 16:04:59.439854   25284 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0603 16:04:59.481853   25284 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0603 16:04:59.509229   25284 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0603 16:04:59.515331   25284 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0603 16:04:59.596512   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:04:59.839967   25284 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0603 16:04:59.890878   25284 certs.go:68] Setting up C:\Users\HHa\.minikube\profiles\minikube for IP: 192.168.49.2
I0603 16:04:59.890878   25284 certs.go:194] generating shared ca certs ...
I0603 16:04:59.891712   25284 certs.go:226] acquiring lock for ca certs: {Name:mk27d4097f3fd18b17020139f6599b3f3b586c6c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0603 16:04:59.899120   25284 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\HHa\.minikube\ca.key
I0603 16:04:59.903526   25284 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\HHa\.minikube\proxy-client-ca.key
I0603 16:04:59.904333   25284 certs.go:256] generating profile certs ...
I0603 16:04:59.911882   25284 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\HHa\.minikube\profiles\minikube\client.key
I0603 16:04:59.914629   25284 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\HHa\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0603 16:04:59.920357   25284 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\HHa\.minikube\profiles\minikube\proxy-client.key
I0603 16:04:59.946223   25284 certs.go:484] found cert: C:\Users\HHa\.minikube\certs\ca-key.pem (1675 bytes)
I0603 16:04:59.948884   25284 certs.go:484] found cert: C:\Users\HHa\.minikube\certs\ca.pem (1070 bytes)
I0603 16:04:59.952020   25284 certs.go:484] found cert: C:\Users\HHa\.minikube\certs\cert.pem (1115 bytes)
I0603 16:04:59.954399   25284 certs.go:484] found cert: C:\Users\HHa\.minikube\certs\key.pem (1675 bytes)
I0603 16:04:59.960996   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0603 16:05:00.029577   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0603 16:05:00.066658   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0603 16:05:00.094965   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0603 16:05:00.251164   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0603 16:05:00.445032   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0603 16:05:00.475727   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0603 16:05:00.568547   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0603 16:05:00.754000   25284 ssh_runner.go:362] scp C:\Users\HHa\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0603 16:05:00.952378   25284 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0603 16:05:00.979850   25284 ssh_runner.go:195] Run: openssl version
I0603 16:05:01.217233   25284 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0603 16:05:01.275924   25284 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0603 16:05:01.344027   25284 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  9 19:38 /usr/share/ca-certificates/minikubeCA.pem
I0603 16:05:01.349900   25284 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0603 16:05:01.414956   25284 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0603 16:05:01.551901   25284 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0603 16:05:01.566840   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0603 16:05:01.670207   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0603 16:05:01.766446   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0603 16:05:01.874435   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0603 16:05:01.972160   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0603 16:05:02.167113   25284 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0603 16:05:02.245827   25284 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8100 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\HHa:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0603 16:05:02.324150   25284 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0603 16:05:02.426110   25284 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0603 16:05:02.440829   25284 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0603 16:05:02.440829   25284 kubeadm.go:593] restartPrimaryControlPlane start ...
I0603 16:05:02.518269   25284 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0603 16:05:02.545871   25284 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0603 16:05:02.593448   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0603 16:05:02.781883   25284 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\HHa\.kube\config
I0603 16:05:02.784519   25284 kubeconfig.go:62] C:\Users\HHa\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0603 16:05:02.785152   25284 lock.go:35] WriteFile acquiring C:\Users\HHa\.kube\config: {Name:mk43a593e7215bd830ca4c9c9a29332452d54bc3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0603 16:05:02.891501   25284 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0603 16:05:02.943832   25284 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0603 16:05:02.943832   25284 kubeadm.go:597] duration metric: took 503.0084ms to restartPrimaryControlPlane
I0603 16:05:02.943832   25284 kubeadm.go:394] duration metric: took 698.0128ms to StartCluster
I0603 16:05:02.943832   25284 settings.go:142] acquiring lock: {Name:mkc13bba152bd036ebe3081999ee0b69282f8193 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0603 16:05:02.944401   25284 settings.go:150] Updating kubeconfig:  C:\Users\HHa\.kube\config
I0603 16:05:02.947232   25284 lock.go:35] WriteFile acquiring C:\Users\HHa\.kube\config: {Name:mk43a593e7215bd830ca4c9c9a29332452d54bc3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0603 16:05:02.950671   25284 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0603 16:05:02.950671   25284 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0603 16:05:02.951225   25284 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0603 16:05:02.951225   25284 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0603 16:05:02.951225   25284 addons.go:247] addon storage-provisioner should already be in state true
I0603 16:05:02.951225   25284 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0603 16:05:02.951225   25284 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0603 16:05:02.951225   25284 out.go:177] 🔎  Verifying Kubernetes components...
I0603 16:05:02.952884   25284 host.go:66] Checking if "minikube" exists ...
I0603 16:05:02.953445   25284 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0603 16:05:03.081391   25284 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0603 16:05:03.218487   25284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0603 16:05:03.219656   25284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0603 16:05:03.351514   25284 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0603 16:05:03.353194   25284 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0603 16:05:03.353194   25284 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0603 16:05:03.387205   25284 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0603 16:05:03.387205   25284 addons.go:247] addon default-storageclass should already be in state true
I0603 16:05:03.387815   25284 host.go:66] Checking if "minikube" exists ...
I0603 16:05:03.395208   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:05:03.483283   25284 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0603 16:05:03.517523   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:05:03.585549   25284 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0603 16:05:03.585549   25284 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0603 16:05:03.622179   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0603 16:05:03.769158   25284 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59420 SSHKeyPath:C:\Users\HHa\.minikube\machines\minikube\id_rsa Username:docker}
I0603 16:05:03.816439   25284 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0603 16:05:04.010325   25284 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0603 16:05:04.125212   25284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0603 16:05:04.151651   25284 api_server.go:52] waiting for apiserver process to appear ...
I0603 16:05:04.220304   25284 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0603 16:05:04.545166   25284 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0603 16:05:14.751903   25284 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (10.5317094s)
I0603 16:05:14.751903   25284 api_server.go:72] duration metric: took 11.8013547s to wait for apiserver process to appear ...
I0603 16:05:14.751903   25284 api_server.go:88] waiting for apiserver healthz status ...
I0603 16:05:14.751903   25284 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59424/healthz ...
I0603 16:05:14.751903   25284 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (10.2068439s)
I0603 16:05:14.751903   25284 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (10.6268019s)
I0603 16:05:14.940211   25284 api_server.go:279] https://127.0.0.1:59424/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0603 16:05:14.940211   25284 api_server.go:103] status: https://127.0.0.1:59424/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0603 16:05:15.142491   25284 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0603 16:05:15.144534   25284 addons.go:514] duration metric: took 12.1939892s for enable addons: enabled=[storage-provisioner default-storageclass]
I0603 16:05:15.252646   25284 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59424/healthz ...
I0603 16:05:15.351987   25284 api_server.go:279] https://127.0.0.1:59424/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0603 16:05:15.352536   25284 api_server.go:103] status: https://127.0.0.1:59424/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0603 16:05:15.752483   25284 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59424/healthz ...
I0603 16:05:15.854938   25284 api_server.go:279] https://127.0.0.1:59424/healthz returned 200:
ok
I0603 16:05:16.041302   25284 api_server.go:141] control plane version: v1.32.0
I0603 16:05:16.041882   25284 api_server.go:131] duration metric: took 1.2899919s to wait for apiserver health ...
I0603 16:05:16.042404   25284 system_pods.go:43] waiting for kube-system pods to appear ...
I0603 16:05:16.161284   25284 system_pods.go:59] 7 kube-system pods found
I0603 16:05:16.161284   25284 system_pods.go:61] "coredns-668d6bf9bc-hndjb" [c923c59e-5e41-4395-9462-95cf52d55fc5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0603 16:05:16.161284   25284 system_pods.go:61] "etcd-minikube" [7f51ba50-6f09-4792-bd08-cbb920a082b6] Running
I0603 16:05:16.161593   25284 system_pods.go:61] "kube-apiserver-minikube" [d3b26447-d2a0-4ded-ac1f-9c7debb5bf6a] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0603 16:05:16.161593   25284 system_pods.go:61] "kube-controller-manager-minikube" [0b32a71b-4d27-42f3-86b3-9741798bb6a0] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0603 16:05:16.161593   25284 system_pods.go:61] "kube-proxy-pdj5s" [627bebaf-400a-4713-b846-09df8095742f] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0603 16:05:16.161593   25284 system_pods.go:61] "kube-scheduler-minikube" [de455daf-cf63-492a-88a3-fc97bb12207d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0603 16:05:16.161593   25284 system_pods.go:61] "storage-provisioner" [8006aa64-68af-4769-8529-602e7f80ddd3] Running
I0603 16:05:16.161593   25284 system_pods.go:74] duration metric: took 119.1905ms to wait for pod list to return data ...
I0603 16:05:16.161593   25284 kubeadm.go:582] duration metric: took 13.2110591s to wait for: map[apiserver:true system_pods:true]
I0603 16:05:16.161698   25284 node_conditions.go:102] verifying NodePressure condition ...
I0603 16:05:16.254901   25284 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0603 16:05:16.254901   25284 node_conditions.go:123] node cpu capacity is 20
I0603 16:05:16.255589   25284 node_conditions.go:105] duration metric: took 93.8913ms to run NodePressure ...
I0603 16:05:16.255589   25284 start.go:241] waiting for startup goroutines ...
I0603 16:05:16.255589   25284 start.go:246] waiting for cluster config update ...
I0603 16:05:16.255649   25284 start.go:255] writing updated cluster config ...
I0603 16:05:16.275066   25284 ssh_runner.go:195] Run: rm -f paused
I0603 16:05:16.632908   25284 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0603 16:05:16.633436   25284 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 03 14:05:00 minikube cri-dockerd[1486]: time="2025-06-03T14:05:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-hndjb_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"56ed82866159f7554aab07b2b25b10705096a0fc3eb6898480b0f0fa15cb86e4\""
Jun 03 14:05:00 minikube cri-dockerd[1486]: time="2025-06-03T14:05:00Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-hndjb_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c2dbed4fe4c9f927d5ed4e1e74ad6a4a2e5fa7d13e862f23eea22a36ce01619d\""
Jun 03 14:05:01 minikube cri-dockerd[1486]: time="2025-06-03T14:05:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/908aa67e14f1700eeaf6ed3bf70995e9b31570615ec9115a15eba5951a6ee6c4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8509b5239b3abf3aa68c0e571db9f8f2e29e659fa17a3901e14807e164b940f0/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-b48f9b6-sptx2_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"df1ac6fde3b6b95eb65649a52b98bab7eb0eaf4b67fdfef34e32af745f1e1caf\""
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ce621fcff462cb72e6ce5a99555b4de5d263c3fafec7e02d1f1f5a8e747e1063/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ecf12b357f8cd9288a52e60189bfd4f7f6ddb2f4d9e9e34f494a43a010c4a85c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-frontend-84d8557576-m57v6_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"400acdcac58635596c2b6ac1c3e0d0e0e172ec5a6774165ef847e98cbe1067bf\""
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-backend-5c6f8db8fd-5rqhw_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"633c7fc883f5a65513d6c0cfe90f977b2441b89ba7ec32f5d96750bc4d2c4cca\""
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-hndjb_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"56ed82866159f7554aab07b2b25b10705096a0fc3eb6898480b0f0fa15cb86e4\""
Jun 03 14:05:02 minikube cri-dockerd[1486]: time="2025-06-03T14:05:02Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-hndjb_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c2dbed4fe4c9f927d5ed4e1e74ad6a4a2e5fa7d13e862f23eea22a36ce01619d\""
Jun 03 14:05:03 minikube cri-dockerd[1486]: time="2025-06-03T14:05:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-frontend-84d8557576-m57v6_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"400acdcac58635596c2b6ac1c3e0d0e0e172ec5a6774165ef847e98cbe1067bf\""
Jun 03 14:05:03 minikube cri-dockerd[1486]: time="2025-06-03T14:05:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-backend-5c6f8db8fd-5rqhw_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"633c7fc883f5a65513d6c0cfe90f977b2441b89ba7ec32f5d96750bc4d2c4cca\""
Jun 03 14:05:03 minikube cri-dockerd[1486]: time="2025-06-03T14:05:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-hndjb_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"56ed82866159f7554aab07b2b25b10705096a0fc3eb6898480b0f0fa15cb86e4\""
Jun 03 14:05:08 minikube cri-dockerd[1486]: time="2025-06-03T14:05:08Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 03 14:05:11 minikube cri-dockerd[1486]: time="2025-06-03T14:05:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5e61f22e5825fc72b99b321c8611a6b89349a7b27e58b84bb84877a557e9d4ce/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:12 minikube cri-dockerd[1486]: time="2025-06-03T14:05:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea56e7eb4daffb67f5b6146c9357538f7f5a08eee7f5d2adf9430a5ab8ea5bbe/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:13 minikube cri-dockerd[1486]: time="2025-06-03T14:05:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dfbf2995cba9128c8a7e8ca5281d8677b6e108eeb168acc0d4a50415d17179f8/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 03 14:05:14 minikube cri-dockerd[1486]: time="2025-06-03T14:05:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f06b59050e57e26c78d4eb328dd69e417bd8e2232604cf4045f918bcc64dcc8d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 03 14:05:14 minikube cri-dockerd[1486]: time="2025-06-03T14:05:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c9d06b02b28f7c7d7c37ec917438dc4a3bbd82ea98c104a5da8bdaa853252f5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 03 14:05:14 minikube cri-dockerd[1486]: time="2025-06-03T14:05:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6567908bd2e8c90d4ee4640c1dabf4e5b80863514d34db9371b753b38ce85d8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 03 14:05:14 minikube cri-dockerd[1486]: time="2025-06-03T14:05:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f4927f65bd4384e241f1067e59039f38364ea4d67cf705f743e56b5e005a16e8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 03 14:05:15 minikube cri-dockerd[1486]: time="2025-06-03T14:05:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/49f286d3478131467639609e69dc3eab22c611e5601f949e28f60edb41e9b369/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.043407911Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.043516698Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.140685228Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.678956507Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.679088898Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:16 minikube dockerd[1167]: time="2025-06-03T14:05:16.841695458Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.392693360Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.392745855Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.397964866Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.605754493Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.605826922Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:34 minikube dockerd[1167]: time="2025-06-03T14:05:34.609929824Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:37 minikube dockerd[1167]: time="2025-06-03T14:05:37.106853831Z" level=info msg="ignoring event" container=2626b53ab13014b2451a953cb8dece1bcb41fc0f6e290378588eb0bf780f0080 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 03 14:05:59 minikube dockerd[1167]: time="2025-06-03T14:05:59.432740549Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:59 minikube dockerd[1167]: time="2025-06-03T14:05:59.432791467Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:05:59 minikube dockerd[1167]: time="2025-06-03T14:05:59.437054888Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:00 minikube dockerd[1167]: time="2025-06-03T14:06:00.483393396Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:00 minikube dockerd[1167]: time="2025-06-03T14:06:00.483431657Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:00 minikube dockerd[1167]: time="2025-06-03T14:06:00.487111782Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:50 minikube dockerd[1167]: time="2025-06-03T14:06:50.429476067Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:50 minikube dockerd[1167]: time="2025-06-03T14:06:50.429615748Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:50 minikube dockerd[1167]: time="2025-06-03T14:06:50.434768291Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:51 minikube dockerd[1167]: time="2025-06-03T14:06:51.406281641Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:51 minikube dockerd[1167]: time="2025-06-03T14:06:51.406351709Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:06:51 minikube dockerd[1167]: time="2025-06-03T14:06:51.412384707Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:15 minikube dockerd[1167]: time="2025-06-03T14:08:15.454654644Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:15 minikube dockerd[1167]: time="2025-06-03T14:08:15.454723823Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:15 minikube dockerd[1167]: time="2025-06-03T14:08:15.459794000Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:21 minikube dockerd[1167]: time="2025-06-03T14:08:21.422633660Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:21 minikube dockerd[1167]: time="2025-06-03T14:08:21.422717973Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:08:21 minikube dockerd[1167]: time="2025-06-03T14:08:21.428553471Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:00 minikube dockerd[1167]: time="2025-06-03T14:11:00.387427621Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:00 minikube dockerd[1167]: time="2025-06-03T14:11:00.387499901Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:00 minikube dockerd[1167]: time="2025-06-03T14:11:00.396450978Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:06 minikube dockerd[1167]: time="2025-06-03T14:11:06.392035740Z" level=warning msg="Error getting v2 registry: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:06 minikube dockerd[1167]: time="2025-06-03T14:11:06.392108559Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"
Jun 03 14:11:06 minikube dockerd[1167]: time="2025-06-03T14:11:06.398230854Z" level=error msg="Handler for POST /v1.43/images/create returned error: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
da1eeea7dde14       6e38f40d628db       7 minutes ago       Running             storage-provisioner       9                   ea56e7eb4daff       storage-provisioner
2b1079b3e8316       117bf5b4421c3       8 minutes ago       Running             app-frontend              2                   f4927f65bd438       app-frontend-84d8557576-m57v6
696906a9717dc       017804e65b414       8 minutes ago       Running             app-backend               2                   a6567908bd2e8       app-backend-5c6f8db8fd-5rqhw
d3b558c7539eb       c69fa2e9cbf5f       8 minutes ago       Running             coredns                   4                   5c9d06b02b28f       coredns-668d6bf9bc-hndjb
711bcd2be025e       a7c5f35e6c6de       8 minutes ago       Running             mongo                     2                   f06b59050e57e       mongo-deployment-b48f9b6-sptx2
2626b53ab1301       6e38f40d628db       8 minutes ago       Exited              storage-provisioner       8                   ea56e7eb4daff       storage-provisioner
5327deda52275       040f9f8aac8cd       8 minutes ago       Running             kube-proxy                4                   5e61f22e5825f       kube-proxy-pdj5s
bf7aa8778bb78       a389e107f4ff1       8 minutes ago       Running             kube-scheduler            4                   ce621fcff462c       kube-scheduler-minikube
1719d9deffab6       c2e17b8d0f4a3       8 minutes ago       Running             kube-apiserver            4                   ecf12b357f8cd       kube-apiserver-minikube
368a6b78fd87a       a9e7e6b294baf       8 minutes ago       Running             etcd                      4                   8509b5239b3ab       etcd-minikube
0ac3968677942       8cab3d2a8bd0f       8 minutes ago       Running             kube-controller-manager   4                   908aa67e14f17       kube-controller-manager-minikube
86b8c5c4b0c80       c69fa2e9cbf5f       48 minutes ago      Exited              coredns                   3                   56ed82866159f       coredns-668d6bf9bc-hndjb
2ff0fe1537192       a7c5f35e6c6de       48 minutes ago      Exited              mongo                     1                   df1ac6fde3b6b       mongo-deployment-b48f9b6-sptx2
5b970eb3af000       117bf5b4421c3       48 minutes ago      Exited              app-frontend              1                   400acdcac5863       app-frontend-84d8557576-m57v6
3900dcefe80b5       017804e65b414       48 minutes ago      Exited              app-backend               1                   633c7fc883f5a       app-backend-5c6f8db8fd-5rqhw
548011b734032       040f9f8aac8cd       48 minutes ago      Exited              kube-proxy                3                   fae1c1d720631       kube-proxy-pdj5s
fd68cff331bc5       a389e107f4ff1       49 minutes ago      Exited              kube-scheduler            3                   9ef4c8aa07f8b       kube-scheduler-minikube
1b9802ae5bdca       a9e7e6b294baf       49 minutes ago      Exited              etcd                      3                   6c743ca4666b0       etcd-minikube
22c18ae4caaa0       c2e17b8d0f4a3       49 minutes ago      Exited              kube-apiserver            3                   1cb0fecf39112       kube-apiserver-minikube
f3322c98871a4       8cab3d2a8bd0f       49 minutes ago      Exited              kube-controller-manager   3                   52c3e00d9e8d2       kube-controller-manager-minikube


==> coredns [86b8c5c4b0c8] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:41704 - 32848 "HINFO IN 5548685285943546036.9166251491693229246. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.18785527s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[130002615]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 13:24:48.036) (total time: 21034ms):
Trace[130002615]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21034ms (13:25:09.066)
Trace[130002615]: [21.034264842s] [21.034264842s] END
[INFO] plugin/kubernetes: Trace[1265646131]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 13:24:48.036) (total time: 21035ms):
Trace[1265646131]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21035ms (13:25:09.067)
Trace[1265646131]: [21.03544205s] [21.03544205s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1580870895]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 13:24:48.036) (total time: 21035ms):
Trace[1580870895]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21035ms (13:25:09.067)
Trace[1580870895]: [21.035262679s] [21.035262679s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 10.244.0.43:35265 - 38702 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000501416s
[INFO] 10.244.0.43:35265 - 38166 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000465856s
[INFO] 10.244.0.43:55589 - 54852 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.00017704s
[INFO] 10.244.0.43:55589 - 54485 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000229295s
[INFO] 10.244.0.43:55359 - 35508 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000175757s
[INFO] 10.244.0.43:55359 - 35181 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000206575s
[INFO] 10.244.0.43:47679 - 30107 "A IN mongo. udp 23 false 512" - - 0 6.003828951s
[INFO] 10.244.0.43:47679 - 30440 "AAAA IN mongo. udp 23 false 512" - - 0 6.004054933s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.46:43021->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.46:58340->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.43:47679 - 30440 "AAAA IN mongo. udp 23 false 512" - - 0 6.003119213s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.46:51826->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.43:47679 - 30107 "A IN mongo. udp 23 false 512" - - 0 6.003026879s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.46:34185->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.43:34810 - 39395 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000565378s
[INFO] 10.244.0.43:34810 - 38924 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.00059476s
[INFO] 10.244.0.43:58946 - 56881 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000182282s
[INFO] 10.244.0.43:58946 - 57446 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000318413s
[INFO] 10.244.0.43:35030 - 34618 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000268618s
[INFO] 10.244.0.43:35030 - 34162 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000361879s
[INFO] 10.244.0.43:44618 - 4888 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.042881077s
[INFO] 10.244.0.43:44618 - 5374 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.063230984s
[INFO] 10.244.0.45:41929 - 15941 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000391203s
[INFO] 10.244.0.45:41929 - 15503 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000522731s
[INFO] 10.244.0.45:51081 - 47191 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000195661s
[INFO] 10.244.0.45:51081 - 46677 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000212723s
[INFO] 10.244.0.45:33427 - 50790 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000151442s
[INFO] 10.244.0.45:33427 - 50386 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000145176s
[INFO] 10.244.0.45:34706 - 62835 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.000155174s
[INFO] 10.244.0.45:34706 - 62559 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000213508s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [d3b558c7539e] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1017694723]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 14:05:20.345) (total time: 21054ms):
Trace[1017694723]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21053ms (14:05:41.396)
Trace[1017694723]: [21.054121377s] [21.054121377s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[2137633641]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 14:05:20.345) (total time: 21054ms):
Trace[2137633641]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21053ms (14:05:41.396)
Trace[2137633641]: [21.054373183s] [21.054373183s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[987132163]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (03-Jun-2025 14:05:20.345) (total time: 21054ms):
Trace[987132163]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21053ms (14:05:41.396)
Trace[987132163]: [21.054269947s] [21.054269947s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 10.244.0.52:36500 - 27925 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.00220922s
[INFO] 10.244.0.52:36500 - 28200 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.002670409s
[INFO] 10.244.0.52:48417 - 56630 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000309125s
[INFO] 10.244.0.52:48417 - 55910 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000424995s
[INFO] 10.244.0.52:35135 - 12348 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000366726s
[INFO] 10.244.0.52:35135 - 11814 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000467473s
[INFO] 10.244.0.52:54153 - 13879 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.0007433s
[INFO] 10.244.0.52:54153 - 12790 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000404154s
[INFO] 10.244.0.52:48803 - 57732 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000312428s
[INFO] 10.244.0.52:48803 - 57030 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000463511s
[INFO] 10.244.0.52:54455 - 49797 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000194661s
[INFO] 10.244.0.52:54455 - 49281 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000221906s
[INFO] 10.244.0.52:48518 - 44406 "AAAA IN mongo. udp 23 false 512" - - 0 6.00237934s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.51:54573->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:48518 - 43895 "A IN mongo. udp 23 false 512" - - 0 6.002554705s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.51:59155->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:48518 - 43895 "A IN mongo. udp 23 false 512" - - 0 6.004014386s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.51:37183->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:48518 - 44406 "AAAA IN mongo. udp 23 false 512" - - 0 6.004446236s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.51:50770->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:43760 - 56703 "AAAA IN mongo. udp 23 false 512" - - 0 6.003344698s
[INFO] 10.244.0.52:43760 - 56386 "A IN mongo. udp 23 false 512" - - 0 6.003312012s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.51:55929->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.51:59898->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:43760 - 56703 "AAAA IN mongo. udp 23 false 512" - - 0 6.002540306s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.51:34066->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:43760 - 56386 "A IN mongo. udp 23 false 512" - - 0 6.002237937s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.51:57858->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.52:58277 - 54701 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000307114s
[INFO] 10.244.0.52:58277 - 54222 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000381122s
[INFO] 10.244.0.52:47613 - 61500 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000163184s
[INFO] 10.244.0.52:47613 - 61128 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000180084s
[INFO] 10.244.0.52:36433 - 64179 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000116721s
[INFO] 10.244.0.52:36433 - 63838 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000169227s
[INFO] 10.244.0.52:56055 - 1962 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 588 0.036521945s
[INFO] 10.244.0.52:56055 - 1512 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.037903787s
[INFO] 10.244.0.53:48883 - 44016 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000616759s
[INFO] 10.244.0.53:48883 - 43269 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000720903s
[INFO] 10.244.0.53:40798 - 29566 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00018498s
[INFO] 10.244.0.53:40798 - 28987 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000218925s
[INFO] 10.244.0.53:48040 - 46620 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.00025833s
[INFO] 10.244.0.53:48040 - 46236 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000294539s
[INFO] 10.244.0.53:59482 - 3390 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000150262s
[INFO] 10.244.0.53:59482 - 3869 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 588 0.001077973s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_09T21_38_52_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 09 May 2025 19:38:49 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 03 Jun 2025 14:13:29 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 03 Jun 2025 14:13:16 +0000   Fri, 09 May 2025 19:38:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 03 Jun 2025 14:13:16 +0000   Fri, 09 May 2025 19:38:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 03 Jun 2025 14:13:16 +0000   Fri, 09 May 2025 19:38:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 03 Jun 2025 14:13:16 +0000   Fri, 09 May 2025 19:38:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16210284Ki
  pods:               110
Allocatable:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16210284Ki
  pods:               110
System Info:
  Machine ID:                 67328d412a984e789f12ad1e80f05f61
  System UUID:                67328d412a984e789f12ad1e80f05f61
  Boot ID:                    68a67564-5dc5-414a-b0a5-4551bca8c072
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     app-backend-5c6f8db8fd-5rqhw                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         22d
  default                     app-frontend-84d8557576-m57v6                0 (0%)        0 (0%)      0 (0%)           0 (0%)         22d
  default                     mongo-deployment-b48f9b6-sptx2               0 (0%)        0 (0%)      0 (0%)           0 (0%)         22d
  ingress-nginx               ingress-nginx-admission-create-4qqlr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  ingress-nginx               ingress-nginx-admission-patch-2wdwd          0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-hzrd9    100m (0%)     0 (0%)      90Mi (0%)        0 (0%)         31m
  kube-system                 coredns-668d6bf9bc-hndjb                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     24d
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         24d
  kube-system                 kube-apiserver-minikube                      250m (1%)     0 (0%)      0 (0%)           0 (0%)         24d
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         24d
  kube-system                 kube-proxy-pdj5s                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24d
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         24d
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         24d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (4%)   0 (0%)
  memory             260Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           48m                    kube-proxy       
  Normal   Starting                           8m18s                  kube-proxy       
  Normal   NodeAllocatableEnforced            49m                    kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           49m                    kubelet          Starting kubelet.
  Warning  CgroupV1                           49m                    kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            49m (x8 over 49m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              49m (x8 over 49m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               49m (x7 over 49m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  49m                    kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  Rebooted                           49m                    kubelet          Node minikube has been rebooted, boot id: 68a67564-5dc5-414a-b0a5-4551bca8c072
  Normal   RegisteredNode                     48m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  8m37s                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m37s                  kubelet          Starting kubelet.
  Warning  CgroupV1                           8m37s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            8m37s (x8 over 8m37s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m37s (x8 over 8m37s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m37s (x7 over 8m37s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            8m37s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     8m17s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun 3 11:26] PCI: Fatal: No config space access function found
[  +0.018705] PCI: System does not support PCI
[  +0.341400] kvm: already loaded the other module
[  +7.757063] FS-Cache: Duplicate cookie detected
[  +0.000783] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.001012] FS-Cache: O-cookie d=00000000206e38f2{9P.session} n=00000000a31e88b0
[  +0.001209] FS-Cache: O-key=[10] '34323934393338313130'
[  +0.000678] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000863] FS-Cache: N-cookie d=00000000206e38f2{9P.session} n=0000000070d9ad20
[  +0.001113] FS-Cache: N-key=[10] '34323934393338313130'
[  +0.464787] FS-Cache: Duplicate cookie detected
[  +0.002613] FS-Cache: O-cookie c=00000008 [p=00000007 fl=226 nc=0 na=1]
[  +0.002519] FS-Cache: O-cookie d=00000000584ba485{9p.inode} n=00000000646e7ec4
[  +0.002854] FS-Cache: O-key=[8] '8132160000006700'
[  +0.002118] FS-Cache: N-cookie c=00000009 [p=00000007 fl=2 nc=0 na=1]
[  +0.002412] FS-Cache: N-cookie d=00000000584ba485{9p.inode} n=00000000f0586930
[  +0.002248] FS-Cache: N-key=[8] '8132160000006700'
[  +0.006698] FS-Cache: Duplicate cookie detected
[  +0.001099] FS-Cache: O-cookie c=00000008 [p=00000007 fl=226 nc=0 na=1]
[  +0.001442] FS-Cache: O-cookie d=00000000584ba485{9p.inode} n=00000000646e7ec4
[  +0.001643] FS-Cache: O-key=[8] '8132160000006700'
[  +0.001316] FS-Cache: N-cookie c=0000000a [p=00000007 fl=2 nc=0 na=1]
[  +0.001288] FS-Cache: N-cookie d=00000000584ba485{9p.inode} n=000000004e9a37c0
[  +0.001597] FS-Cache: N-key=[8] '8132160000006700'
[  +0.382111] FS-Cache: Duplicate cookie detected
[  +0.002069] FS-Cache: O-cookie c=0000000c [p=00000007 fl=226 nc=0 na=1]
[  +0.002591] FS-Cache: O-cookie d=00000000584ba485{9p.inode} n=00000000878f854a
[  +0.002916] FS-Cache: O-key=[8] '215d160000004200'
[  +0.001865] FS-Cache: N-cookie c=0000000d [p=00000007 fl=2 nc=0 na=1]
[  +0.001564] FS-Cache: N-cookie d=00000000584ba485{9p.inode} n=0000000091621beb
[  +0.001704] FS-Cache: N-key=[8] '215d160000004200'
[  +0.652803] FS-Cache: Duplicate cookie detected
[  +0.002090] FS-Cache: O-cookie c=0000000f [p=00000007 fl=226 nc=0 na=1]
[  +0.001668] FS-Cache: O-cookie d=00000000584ba485{9p.inode} n=000000009f8c36d6
[  +0.002137] FS-Cache: O-key=[8] '5161160000004a00'
[  +0.001765] FS-Cache: N-cookie c=00000010 [p=00000007 fl=2 nc=0 na=1]
[  +0.001738] FS-Cache: N-cookie d=00000000584ba485{9p.inode} n=0000000039d0410d
[  +0.001911] FS-Cache: N-key=[8] '5161160000004a00'
[  +0.120789] FS-Cache: Duplicate cookie detected
[  +0.001368] FS-Cache: O-cookie c=00000012 [p=00000007 fl=226 nc=0 na=1]
[  +0.001117] FS-Cache: O-cookie d=00000000584ba485{9p.inode} n=00000000f5ec4f28
[  +0.001986] FS-Cache: O-key=[8] 'f6720e0000005d00'
[  +0.000868] FS-Cache: N-cookie c=00000013 [p=00000007 fl=2 nc=0 na=1]
[  +0.001072] FS-Cache: N-cookie d=00000000584ba485{9p.inode} n=000000007cd1e2f8
[  +0.001367] FS-Cache: N-key=[8] 'f6720e0000005d00'
[  +0.554268] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.021122] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Berlin not found. Is the tzdata package installed?
[  +0.285464] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.019963] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002377] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000878] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001438] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.456750] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001120] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000962] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001688] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.658259] WSL (256) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.140592] netlink: 'init': attribute type 4 has an invalid length.
[Jun 3 13:24] tmpfs: Unknown parameter 'noswap'
[Jun 3 14:04] tmpfs: Unknown parameter 'noswap'


==> etcd [1b9802ae5bdc] <==
{"level":"info","ts":"2025-06-03T13:24:34.673271Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-03T13:24:34.673445Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-03T13:24:34.673584Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-03T13:24:34.674381Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-03T13:24:34.674546Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-03T13:24:34.679328Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-03T13:24:34.680056Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-03T13:24:34.680555Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-03T13:24:34.681996Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-03T13:24:41.753472Z","caller":"traceutil/trace.go:171","msg":"trace[725613907] transaction","detail":"{read_only:false; response_revision:13508; number_of_response:1; }","duration":"102.578893ms","start":"2025-06-03T13:24:41.650636Z","end":"2025-06-03T13:24:41.753215Z","steps":["trace[725613907] 'process raft request'  (duration: 88.77952ms)","trace[725613907] 'compare'  (duration: 12.341102ms)"],"step_count":2}
{"level":"info","ts":"2025-06-03T13:24:41.949582Z","caller":"traceutil/trace.go:171","msg":"trace[2016810706] transaction","detail":"{read_only:false; response_revision:13511; number_of_response:1; }","duration":"108.17248ms","start":"2025-06-03T13:24:41.841330Z","end":"2025-06-03T13:24:41.949503Z","steps":["trace[2016810706] 'process raft request'  (duration: 94.803567ms)","trace[2016810706] 'compare'  (duration: 12.526716ms)"],"step_count":2}
{"level":"info","ts":"2025-06-03T13:24:47.236412Z","caller":"traceutil/trace.go:171","msg":"trace[1894879896] transaction","detail":"{read_only:false; response_revision:13546; number_of_response:1; }","duration":"173.828229ms","start":"2025-06-03T13:24:47.062562Z","end":"2025-06-03T13:24:47.236391Z","steps":["trace[1894879896] 'process raft request'  (duration: 173.520456ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:24:47.236494Z","caller":"traceutil/trace.go:171","msg":"trace[374275515] transaction","detail":"{read_only:false; response_revision:13547; number_of_response:1; }","duration":"173.313877ms","start":"2025-06-03T13:24:47.063165Z","end":"2025-06-03T13:24:47.236479Z","steps":["trace[374275515] 'process raft request'  (duration: 172.993765ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:24:47.245566Z","caller":"traceutil/trace.go:171","msg":"trace[1410467447] transaction","detail":"{read_only:false; response_revision:13548; number_of_response:1; }","duration":"103.076088ms","start":"2025-06-03T13:24:47.142455Z","end":"2025-06-03T13:24:47.245532Z","steps":["trace[1410467447] 'process raft request'  (duration: 96.788827ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:34:35.675417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13784}
{"level":"info","ts":"2025-06-03T13:34:35.687230Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13784,"took":"11.072206ms","hash":858209021,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2543616,"current-db-size-in-use":"2.5 MB"}
{"level":"info","ts":"2025-06-03T13:34:35.687417Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":858209021,"revision":13784,"compact-revision":13148}
{"level":"info","ts":"2025-06-03T13:39:35.648453Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14023}
{"level":"info","ts":"2025-06-03T13:39:35.656899Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14023,"took":"7.883207ms","hash":954868384,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1622016,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-06-03T13:39:35.657046Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":954868384,"revision":14023,"compact-revision":13784}
{"level":"warn","ts":"2025-06-03T13:42:37.881480Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.848113ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-03T13:42:37.881758Z","caller":"traceutil/trace.go:171","msg":"trace[97125585] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14450; }","duration":"106.142665ms","start":"2025-06-03T13:42:37.775580Z","end":"2025-06-03T13:42:37.881723Z","steps":["trace[97125585] 'range keys from in-memory index tree'  (duration: 105.826689ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T13:42:37.882610Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.163779ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037685531231416 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/ingress-nginx/ingress-nginx-admission-patch-2wdwd\" mod_revision:14449 > success:<request_put:<key:\"/registry/pods/ingress-nginx/ingress-nginx-admission-patch-2wdwd\" value_size:2825 >> failure:<request_range:<key:\"/registry/pods/ingress-nginx/ingress-nginx-admission-patch-2wdwd\" > >>","response":"size:16"}
{"level":"info","ts":"2025-06-03T13:42:37.882811Z","caller":"traceutil/trace.go:171","msg":"trace[942508749] linearizableReadLoop","detail":"{readStateIndex:17673; appliedIndex:17672; }","duration":"207.138114ms","start":"2025-06-03T13:42:37.675655Z","end":"2025-06-03T13:42:37.882793Z","steps":["trace[942508749] 'read index received'  (duration: 98.889674ms)","trace[942508749] 'applied index is now lower than readState.Index'  (duration: 108.24701ms)"],"step_count":2}
{"level":"info","ts":"2025-06-03T13:42:37.882952Z","caller":"traceutil/trace.go:171","msg":"trace[1957006865] transaction","detail":"{read_only:false; response_revision:14451; number_of_response:1; }","duration":"207.580299ms","start":"2025-06-03T13:42:37.675360Z","end":"2025-06-03T13:42:37.882940Z","steps":["trace[1957006865] 'process raft request'  (duration: 99.053602ms)","trace[1957006865] 'compare'  (duration: 107.053286ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-03T13:42:37.883286Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.601545ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/ingress-nginx/ingress-nginx-controller-56d7c84fd4\" limit:1 ","response":"range_response_count:1 size:5258"}
{"level":"info","ts":"2025-06-03T13:42:37.883466Z","caller":"traceutil/trace.go:171","msg":"trace[1643073455] range","detail":"{range_begin:/registry/replicasets/ingress-nginx/ingress-nginx-controller-56d7c84fd4; range_end:; response_count:1; response_revision:14451; }","duration":"207.85248ms","start":"2025-06-03T13:42:37.675585Z","end":"2025-06-03T13:42:37.883437Z","steps":["trace[1643073455] 'agreement among raft nodes before linearized reading'  (duration: 207.499832ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.884151Z","caller":"traceutil/trace.go:171","msg":"trace[1340921007] transaction","detail":"{read_only:false; response_revision:14452; number_of_response:1; }","duration":"207.15063ms","start":"2025-06-03T13:42:37.676976Z","end":"2025-06-03T13:42:37.884127Z","steps":["trace[1340921007] 'process raft request'  (duration: 206.959907ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.884240Z","caller":"traceutil/trace.go:171","msg":"trace[1919617707] transaction","detail":"{read_only:false; response_revision:14453; number_of_response:1; }","duration":"205.801701ms","start":"2025-06-03T13:42:37.678400Z","end":"2025-06-03T13:42:37.884201Z","steps":["trace[1919617707] 'process raft request'  (duration: 205.661652ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.884288Z","caller":"traceutil/trace.go:171","msg":"trace[365331499] transaction","detail":"{read_only:false; response_revision:14455; number_of_response:1; }","duration":"199.264463ms","start":"2025-06-03T13:42:37.685003Z","end":"2025-06-03T13:42:37.884268Z","steps":["trace[365331499] 'process raft request'  (duration: 199.17164ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.884494Z","caller":"traceutil/trace.go:171","msg":"trace[1425101046] transaction","detail":"{read_only:false; response_revision:14454; number_of_response:1; }","duration":"204.672462ms","start":"2025-06-03T13:42:37.679787Z","end":"2025-06-03T13:42:37.884459Z","steps":["trace[1425101046] 'process raft request'  (duration: 204.33035ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.975056Z","caller":"traceutil/trace.go:171","msg":"trace[95588407] transaction","detail":"{read_only:false; response_revision:14456; number_of_response:1; }","duration":"200.129863ms","start":"2025-06-03T13:42:37.774856Z","end":"2025-06-03T13:42:37.974986Z","steps":["trace[95588407] 'process raft request'  (duration: 199.464326ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.975563Z","caller":"traceutil/trace.go:171","msg":"trace[1401772185] transaction","detail":"{read_only:false; response_revision:14457; number_of_response:1; }","duration":"101.235521ms","start":"2025-06-03T13:42:37.874287Z","end":"2025-06-03T13:42:37.975523Z","steps":["trace[1401772185] 'process raft request'  (duration: 100.402496ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T13:42:37.975695Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"297.452077ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-06-03T13:42:37.975791Z","caller":"traceutil/trace.go:171","msg":"trace[1782157336] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:14459; }","duration":"297.625166ms","start":"2025-06-03T13:42:37.678136Z","end":"2025-06-03T13:42:37.975761Z","steps":["trace[1782157336] 'agreement among raft nodes before linearized reading'  (duration: 297.281757ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:42:37.991221Z","caller":"traceutil/trace.go:171","msg":"trace[336325149] transaction","detail":"{read_only:false; response_revision:14460; number_of_response:1; }","duration":"102.438363ms","start":"2025-06-03T13:42:37.888736Z","end":"2025-06-03T13:42:37.991174Z","steps":["trace[336325149] 'process raft request'  (duration: 93.060088ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T13:42:37.991548Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.267711ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/ingress-nginx/ingress-nginx-controller-56d7c84fd4\" limit:1 ","response":"range_response_count:1 size:5258"}
{"level":"info","ts":"2025-06-03T13:42:37.991644Z","caller":"traceutil/trace.go:171","msg":"trace[1129851484] range","detail":"{range_begin:/registry/replicasets/ingress-nginx/ingress-nginx-controller-56d7c84fd4; range_end:; response_count:1; response_revision:14461; }","duration":"102.424359ms","start":"2025-06-03T13:42:37.889194Z","end":"2025-06-03T13:42:37.991619Z","steps":["trace[1129851484] 'agreement among raft nodes before linearized reading'  (duration: 102.136341ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T13:44:35.633511Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14264}
{"level":"info","ts":"2025-06-03T13:44:35.641423Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14264,"took":"7.297362ms","hash":2971825040,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2203648,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-06-03T13:44:35.641554Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2971825040,"revision":14264,"compact-revision":14023}
{"level":"info","ts":"2025-06-03T13:49:35.614863Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14629}
{"level":"info","ts":"2025-06-03T13:49:35.621300Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14629,"took":"5.983458ms","hash":3846619853,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2371584,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-06-03T13:49:35.621373Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3846619853,"revision":14629,"compact-revision":14264}
{"level":"info","ts":"2025-06-03T13:54:35.595337Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14936}
{"level":"info","ts":"2025-06-03T13:54:35.598411Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14936,"took":"2.783289ms","hash":2473714552,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1925120,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-06-03T13:54:35.598457Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2473714552,"revision":14936,"compact-revision":14629}
{"level":"info","ts":"2025-06-03T13:59:35.577685Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15188}
{"level":"info","ts":"2025-06-03T13:59:35.583612Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":15188,"took":"5.074106ms","hash":1021746708,"current-db-size-bytes":3948544,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":1822720,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-06-03T13:59:35.583705Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1021746708,"revision":15188,"compact-revision":14936}
{"level":"info","ts":"2025-06-03T14:03:34.854892Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-06-03T14:03:34.855517Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-06-03T14:03:34.946905Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-03T14:03:34.947352Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-03T14:03:35.146129Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-03T14:03:35.146198Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-06-03T14:03:35.146376Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-03T14:03:35.257120Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-03T14:03:35.257688Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-03T14:03:35.257714Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [368a6b78fd87] <==
{"level":"info","ts":"2025-06-03T14:05:04.542478Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-03T14:05:04.545784Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-06-03T14:05:04.546315Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-06-03T14:05:04.546397Z","caller":"etcdserver/server.go:864","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-03T14:05:04.546756Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-03T14:05:04.546978Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-03T14:05:04.547064Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-03T14:05:04.547074Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-03T14:05:04.547498Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-03T14:05:04.550180Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-03T14:05:04.550370Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-03T14:05:04.550418Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-03T14:05:04.550688Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-03T14:05:04.550813Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-03T14:05:04.654630Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2025-06-03T14:05:04.654769Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2025-06-03T14:05:04.654890Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2025-06-03T14:05:04.654924Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2025-06-03T14:05:04.654937Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-06-03T14:05:04.654957Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2025-06-03T14:05:04.654973Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2025-06-03T14:05:04.669039Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-03T14:05:04.669234Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-03T14:05:04.669529Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-03T14:05:04.671720Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-03T14:05:04.672850Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-03T14:05:04.673800Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-03T14:05:04.674724Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-03T14:05:04.737902Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-03T14:05:04.744206Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-03T14:05:09.052033Z","caller":"traceutil/trace.go:171","msg":"trace[1079458586] transaction","detail":"{read_only:false; response_revision:15643; number_of_response:1; }","duration":"103.202316ms","start":"2025-06-03T14:05:08.948759Z","end":"2025-06-03T14:05:09.051961Z","steps":["trace[1079458586] 'process raft request'  (duration: 102.255407ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:11.753013Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.749893ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037686153332250 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.18458d662b450849\" mod_revision:15661 > success:<request_put:<key:\"/registry/events/default/minikube.18458d662b450849\" value_size:568 lease:8128037686153332199 >> failure:<request_range:<key:\"/registry/events/default/minikube.18458d662b450849\" > >>","response":"size:16"}
{"level":"info","ts":"2025-06-03T14:05:11.753643Z","caller":"traceutil/trace.go:171","msg":"trace[705423921] transaction","detail":"{read_only:false; response_revision:15664; number_of_response:1; }","duration":"107.144018ms","start":"2025-06-03T14:05:11.646468Z","end":"2025-06-03T14:05:11.753612Z","steps":["trace[705423921] 'compare'  (duration: 101.561136ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:12.049930Z","caller":"traceutil/trace.go:171","msg":"trace[258636752] linearizableReadLoop","detail":"{readStateIndex:19163; appliedIndex:19162; }","duration":"104.025991ms","start":"2025-06-03T14:05:11.945858Z","end":"2025-06-03T14:05:12.049884Z","steps":["trace[258636752] 'read index received'  (duration: 103.512224ms)","trace[258636752] 'applied index is now lower than readState.Index'  (duration: 512.24µs)"],"step_count":2}
{"level":"info","ts":"2025-06-03T14:05:12.050218Z","caller":"traceutil/trace.go:171","msg":"trace[569492365] transaction","detail":"{read_only:false; response_revision:15665; number_of_response:1; }","duration":"111.601815ms","start":"2025-06-03T14:05:11.938557Z","end":"2025-06-03T14:05:12.050159Z","steps":["trace[569492365] 'process raft request'  (duration: 110.948173ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:12.050438Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.496768ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:kube-controller-manager\" limit:1 ","response":"range_response_count:1 size:745"}
{"level":"info","ts":"2025-06-03T14:05:12.050552Z","caller":"traceutil/trace.go:171","msg":"trace[1428447901] range","detail":"{range_begin:/registry/clusterrolebindings/system:kube-controller-manager; range_end:; response_count:1; response_revision:15665; }","duration":"104.724273ms","start":"2025-06-03T14:05:11.945793Z","end":"2025-06-03T14:05:12.050518Z","steps":["trace[1428447901] 'agreement among raft nodes before linearized reading'  (duration: 104.35209ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:12.749680Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.414489ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037686153332283 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.18458d662b44dd14\" mod_revision:15668 > success:<request_put:<key:\"/registry/events/default/minikube.18458d662b44dd14\" value_size:574 lease:8128037686153332199 >> failure:<request_range:<key:\"/registry/events/default/minikube.18458d662b44dd14\" > >>","response":"size:16"}
{"level":"info","ts":"2025-06-03T14:05:12.750345Z","caller":"traceutil/trace.go:171","msg":"trace[1960768670] linearizableReadLoop","detail":"{readStateIndex:19169; appliedIndex:19168; }","duration":"106.656036ms","start":"2025-06-03T14:05:12.643639Z","end":"2025-06-03T14:05:12.750295Z","steps":["trace[1960768670] 'read index received'  (duration: 2.033984ms)","trace[1960768670] 'applied index is now lower than readState.Index'  (duration: 104.617535ms)"],"step_count":2}
{"level":"info","ts":"2025-06-03T14:05:12.750627Z","caller":"traceutil/trace.go:171","msg":"trace[1802294367] transaction","detail":"{read_only:false; response_revision:15671; number_of_response:1; }","duration":"107.622003ms","start":"2025-06-03T14:05:12.642809Z","end":"2025-06-03T14:05:12.750431Z","steps":["trace[1802294367] 'compare'  (duration: 103.166451ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:12.750662Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.980055ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:endpointslicemirroring-controller\" limit:1 ","response":"range_response_count:1 size:807"}
{"level":"info","ts":"2025-06-03T14:05:12.750890Z","caller":"traceutil/trace.go:171","msg":"trace[1751495488] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:endpointslicemirroring-controller; range_end:; response_count:1; response_revision:15671; }","duration":"107.280449ms","start":"2025-06-03T14:05:12.643565Z","end":"2025-06-03T14:05:12.750846Z","steps":["trace[1751495488] 'agreement among raft nodes before linearized reading'  (duration: 106.880254ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:13.144126Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.453571ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/system:controller:generic-garbage-collector\" limit:1 ","response":"range_response_count:1 size:775"}
{"level":"info","ts":"2025-06-03T14:05:13.144526Z","caller":"traceutil/trace.go:171","msg":"trace[556956992] range","detail":"{range_begin:/registry/clusterrolebindings/system:controller:generic-garbage-collector; range_end:; response_count:1; response_revision:15673; }","duration":"101.89725ms","start":"2025-06-03T14:05:13.042486Z","end":"2025-06-03T14:05:13.144383Z","steps":["trace[556956992] 'agreement among raft nodes before linearized reading'  (duration: 95.684286ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:14.052368Z","caller":"traceutil/trace.go:171","msg":"trace[1002425559] transaction","detail":"{read_only:false; response_revision:15679; number_of_response:1; }","duration":"108.394002ms","start":"2025-06-03T14:05:13.943932Z","end":"2025-06-03T14:05:14.052326Z","steps":["trace[1002425559] 'process raft request'  (duration: 106.542851ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:14.446956Z","caller":"traceutil/trace.go:171","msg":"trace[1559434863] transaction","detail":"{read_only:false; response_revision:15682; number_of_response:1; }","duration":"100.015992ms","start":"2025-06-03T14:05:14.346873Z","end":"2025-06-03T14:05:14.446889Z","steps":["trace[1559434863] 'process raft request'  (duration: 99.446412ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:14.538776Z","caller":"traceutil/trace.go:171","msg":"trace[89240562] transaction","detail":"{read_only:false; response_revision:15683; number_of_response:1; }","duration":"100.532888ms","start":"2025-06-03T14:05:14.438171Z","end":"2025-06-03T14:05:14.538704Z","steps":["trace[89240562] 'process raft request'  (duration: 100.188268ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:15.445788Z","caller":"traceutil/trace.go:171","msg":"trace[1968548441] transaction","detail":"{read_only:false; response_revision:15690; number_of_response:1; }","duration":"103.374776ms","start":"2025-06-03T14:05:15.342159Z","end":"2025-06-03T14:05:15.445533Z","steps":["trace[1968548441] 'process raft request'  (duration: 12.429144ms)","trace[1968548441] 'compare'  (duration: 90.485925ms)"],"step_count":2}
{"level":"info","ts":"2025-06-03T14:05:15.642513Z","caller":"traceutil/trace.go:171","msg":"trace[433805092] transaction","detail":"{read_only:false; response_revision:15692; number_of_response:1; }","duration":"100.325186ms","start":"2025-06-03T14:05:15.542130Z","end":"2025-06-03T14:05:15.642455Z","steps":["trace[433805092] 'process raft request'  (duration: 13.751924ms)","trace[433805092] 'compare'  (duration: 86.112212ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-03T14:05:15.642935Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.32065ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system:controller:bootstrap-signer\" limit:1 ","response":"range_response_count:1 size:741"}
{"level":"info","ts":"2025-06-03T14:05:15.643049Z","caller":"traceutil/trace.go:171","msg":"trace[1232900293] range","detail":"{range_begin:/registry/rolebindings/kube-system/system:controller:bootstrap-signer; range_end:; response_count:1; response_revision:15693; }","duration":"100.475521ms","start":"2025-06-03T14:05:15.542545Z","end":"2025-06-03T14:05:15.643021Z","steps":["trace[1232900293] 'agreement among raft nodes before linearized reading'  (duration: 100.029519ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:19.045119Z","caller":"traceutil/trace.go:171","msg":"trace[2097230646] transaction","detail":"{read_only:false; response_revision:15739; number_of_response:1; }","duration":"100.193788ms","start":"2025-06-03T14:05:18.944888Z","end":"2025-06-03T14:05:19.045082Z","steps":["trace[2097230646] 'process raft request'  (duration: 99.745486ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:19.045712Z","caller":"traceutil/trace.go:171","msg":"trace[102031930] transaction","detail":"{read_only:false; response_revision:15740; number_of_response:1; }","duration":"100.846573ms","start":"2025-06-03T14:05:18.944823Z","end":"2025-06-03T14:05:19.045670Z","steps":["trace[102031930] 'process raft request'  (duration: 100.153725ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:19.054343Z","caller":"traceutil/trace.go:171","msg":"trace[1233990515] transaction","detail":"{read_only:false; response_revision:15741; number_of_response:1; }","duration":"103.091876ms","start":"2025-06-03T14:05:18.951218Z","end":"2025-06-03T14:05:19.054310Z","steps":["trace[1233990515] 'process raft request'  (duration: 102.32224ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-03T14:05:21.337173Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.491736ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037686153332563 > lease_revoke:<id:70cc9735f713e34a>","response":"size:29"}
{"level":"info","ts":"2025-06-03T14:05:21.337546Z","caller":"traceutil/trace.go:171","msg":"trace[1928934717] linearizableReadLoop","detail":"{readStateIndex:19259; appliedIndex:19258; }","duration":"101.15502ms","start":"2025-06-03T14:05:21.236362Z","end":"2025-06-03T14:05:21.337517Z","steps":["trace[1928934717] 'read index received'  (duration: 446.713µs)","trace[1928934717] 'applied index is now lower than readState.Index'  (duration: 100.705386ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-03T14:05:21.337824Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.424984ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner\" limit:1 ","response":"range_response_count:1 size:238"}
{"level":"warn","ts":"2025-06-03T14:05:21.337829Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.424548ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/endpoint-controller\" limit:1 ","response":"range_response_count:1 size:203"}
{"level":"info","ts":"2025-06-03T14:05:21.337985Z","caller":"traceutil/trace.go:171","msg":"trace[986625659] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner; range_end:; response_count:1; response_revision:15762; }","duration":"101.635888ms","start":"2025-06-03T14:05:21.236319Z","end":"2025-06-03T14:05:21.337955Z","steps":["trace[986625659] 'agreement among raft nodes before linearized reading'  (duration: 101.3218ms)"],"step_count":1}
{"level":"info","ts":"2025-06-03T14:05:21.338008Z","caller":"traceutil/trace.go:171","msg":"trace[1283302636] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/endpoint-controller; range_end:; response_count:1; response_revision:15762; }","duration":"101.654734ms","start":"2025-06-03T14:05:21.236329Z","end":"2025-06-03T14:05:21.337984Z","steps":["trace[1283302636] 'agreement among raft nodes before linearized reading'  (duration: 101.308042ms)"],"step_count":1}


==> kernel <==
 14:13:37 up  2:47,  0 users,  load average: 0.27, 0.45, 0.50
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [1719d9deffab] <==
I0603 14:05:06.904598       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0603 14:05:06.904634       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0603 14:05:07.492178       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0603 14:05:07.492196       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0603 14:05:07.492515       1 secure_serving.go:213] Serving securely on [::]:8443
I0603 14:05:07.492614       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0603 14:05:07.492623       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0603 14:05:07.493015       1 controller.go:78] Starting OpenAPI AggregationController
I0603 14:05:07.493169       1 aggregator.go:169] waiting for initial CRD sync...
I0603 14:05:07.493226       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0603 14:05:07.493318       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0603 14:05:07.493676       1 controller.go:119] Starting legacy_token_tracking_controller
I0603 14:05:07.493711       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0603 14:05:07.493724       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0603 14:05:07.493742       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0603 14:05:07.493018       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0603 14:05:07.493810       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0603 14:05:07.493830       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0603 14:05:07.494019       1 local_available_controller.go:156] Starting LocalAvailability controller
I0603 14:05:07.494066       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0603 14:05:07.494011       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0603 14:05:07.494082       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0603 14:05:07.494393       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0603 14:05:07.494521       1 controller.go:142] Starting OpenAPI controller
I0603 14:05:07.494695       1 controller.go:90] Starting OpenAPI V3 controller
I0603 14:05:07.494772       1 naming_controller.go:294] Starting NamingConditionController
I0603 14:05:07.494811       1 establishing_controller.go:81] Starting EstablishingController
I0603 14:05:07.494859       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0603 14:05:07.494910       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0603 14:05:07.494924       1 crd_finalizer.go:269] Starting CRDFinalizer
I0603 14:05:07.502487       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0603 14:05:07.502682       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0603 14:05:07.528878       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0603 14:05:07.528978       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0603 14:05:07.529042       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0603 14:05:07.529194       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0603 14:05:07.640209       1 shared_informer.go:320] Caches are synced for configmaps
I0603 14:05:07.738006       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0603 14:05:07.738201       1 aggregator.go:171] initial CRD sync complete...
I0603 14:05:07.738512       1 autoregister_controller.go:144] Starting autoregister controller
I0603 14:05:07.738554       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0603 14:05:07.741922       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0603 14:05:07.838206       1 shared_informer.go:320] Caches are synced for node_authorizer
I0603 14:05:07.838416       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0603 14:05:07.838434       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0603 14:05:07.838440       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0603 14:05:07.838578       1 policy_source.go:240] refreshing policies
I0603 14:05:07.841609       1 cache.go:39] Caches are synced for autoregister controller
I0603 14:05:07.937186       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0603 14:05:07.937391       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0603 14:05:07.937391       1 cache.go:39] Caches are synced for LocalAvailability controller
I0603 14:05:07.937396       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0603 14:05:07.953335       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0603 14:05:08.038165       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0603 14:05:08.347654       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0603 14:05:08.549958       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0603 14:05:21.240312       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0603 14:05:21.244043       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0603 14:05:21.437281       1 controller.go:615] quota admission added evaluator for: endpoints
I0603 14:05:21.547303       1 controller.go:615] quota admission added evaluator for: deployments.apps


==> kube-apiserver [22c18ae4caaa] <==
W0603 14:03:40.624111       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.640801       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.656189       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.722901       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.727479       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.779640       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.782515       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.800412       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.884341       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:40.885739       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.065191       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.257459       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.419177       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.460898       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.578538       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.586510       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.635699       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.658995       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.679424       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.693206       1 logging.go:55] [core] [Channel #18 SubChannel #19]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.704891       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.706503       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.769837       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.790883       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.826725       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.878433       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.899783       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.915068       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.939352       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.940352       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.993867       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:43.994065       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.041389       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.067964       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.076295       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.088597       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.096161       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.195396       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.230749       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.238376       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.258040       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.270926       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.274724       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.317706       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.367979       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.400084       1 logging.go:55] [core] [Channel #2 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.580553       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.593620       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.701036       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.708696       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.847079       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.856128       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.900009       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.927916       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.932869       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.945934       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.953836       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:44.990288       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:45.010020       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 14:03:45.030481       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [0ac396867794] <==
I0603 14:05:20.944556       1 shared_informer.go:320] Caches are synced for job
I0603 14:05:20.945612       1 shared_informer.go:320] Caches are synced for attach detach
I0603 14:05:20.946411       1 shared_informer.go:320] Caches are synced for taint
I0603 14:05:20.946608       1 shared_informer.go:320] Caches are synced for HPA
I0603 14:05:20.946929       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0603 14:05:20.947249       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0603 14:05:20.947567       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0603 14:05:20.948438       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0603 14:05:20.948537       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0603 14:05:21.034367       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0603 14:05:21.034377       1 shared_informer.go:320] Caches are synced for disruption
I0603 14:05:21.034459       1 shared_informer.go:320] Caches are synced for resource quota
I0603 14:05:21.034479       1 shared_informer.go:320] Caches are synced for stateful set
I0603 14:05:21.034583       1 shared_informer.go:320] Caches are synced for GC
I0603 14:05:21.034645       1 shared_informer.go:320] Caches are synced for endpoint
I0603 14:05:21.034654       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0603 14:05:21.034674       1 shared_informer.go:320] Caches are synced for PVC protection
I0603 14:05:21.034958       1 shared_informer.go:320] Caches are synced for deployment
I0603 14:05:21.036967       1 shared_informer.go:320] Caches are synced for resource quota
I0603 14:05:21.037143       1 shared_informer.go:320] Caches are synced for persistent volume
I0603 14:05:21.039320       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0603 14:05:21.243985       1 shared_informer.go:320] Caches are synced for garbage collector
I0603 14:05:21.244181       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0603 14:05:21.244209       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0603 14:05:21.246929       1 shared_informer.go:320] Caches are synced for garbage collector
I0603 14:05:21.435275       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-backend-5c6f8db8fd" duration="494.596396ms"
I0603 14:05:21.435470       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-backend-5c6f8db8fd" duration="81.846µs"
I0603 14:05:21.435488       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="495.292148ms"
I0603 14:05:21.435616       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="59.888µs"
I0603 14:05:21.440217       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="499.914596ms"
I0603 14:05:21.440532       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="182.871µs"
I0603 14:05:22.037880       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="93.884655ms"
I0603 14:05:22.038213       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="106.455µs"
I0603 14:05:22.247093       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-backend-5c6f8db8fd" duration="100.147ms"
I0603 14:05:22.248453       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-backend-5c6f8db8fd" duration="165.564µs"
I0603 14:05:22.436205       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:05:43.289697       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="13.922328ms"
I0603 14:05:43.289975       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="104.127µs"
I0603 14:05:46.285107       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:05:47.273459       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:05:59.274455       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:06:00.277508       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:06:13.268478       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:06:15.272241       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:06:26.271377       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:06:27.270542       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:07:01.258214       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:07:05.263301       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:07:14.267147       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:07:17.269617       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:08:11.977409       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 14:08:30.271560       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:08:33.265252       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:08:44.254946       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:08:45.327750       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:11:15.245166       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:11:20.242348       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:11:30.242447       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 14:11:31.239322       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:13:16.859255       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [f3322c98871a] <==
I0603 13:24:47.240415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-b48f9b6" duration="184.438074ms"
I0603 13:24:47.240601       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-b48f9b6" duration="65.591µs"
I0603 13:24:47.346606       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="95.272µs"
I0603 13:24:48.654960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="90.083544ms"
I0603 13:24:48.655906       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-frontend-84d8557576" duration="626.247µs"
I0603 13:25:18.557076       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="58.617838ms"
I0603 13:25:18.558015       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="367.311µs"
I0603 13:30:54.905658       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:36:00.466218       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:41:06.442791       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:42:37.509004       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0603 13:42:37.535821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="41.424002ms"
I0603 13:42:37.540810       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0603 13:42:37.543009       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:42:37.593409       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="57.50569ms"
I0603 13:42:37.595693       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:37.595945       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:42:37.596358       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:42:37.884465       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:37.887882       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:38.075058       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="481.507619ms"
I0603 13:42:38.075210       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:42:38.075388       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="189.957µs"
I0603 13:42:38.105502       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:39.139020       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:42:40.158738       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:52.108769       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:42:52.141290       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:43:05.107460       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:43:05.142339       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:43:17.092577       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:43:20.106015       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:43:30.104131       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:43:36.102798       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:43:44.100620       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:43:48.096481       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:44:13.087377       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:44:26.089571       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:44:27.094775       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:44:39.097009       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:45:40.070798       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:45:53.073983       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:45:55.080594       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:46:07.074826       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:46:11.843068       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:48:29.057800       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:48:42.065170       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:48:43.059622       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:48:55.047491       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:51:17.637048       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:53:33.024598       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:53:47.025998       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:53:54.015512       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:54:08.031646       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:56:22.952717       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0603 13:58:39.002631       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:58:51.981838       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0603 13:58:57.994047       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 13:59:12.983264       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0603 14:01:29.070514       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [5327deda5227] <==
I0603 14:05:16.755950       1 server_linux.go:66] "Using iptables proxy"
I0603 14:05:17.750368       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0603 14:05:17.750863       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0603 14:05:18.347321       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0603 14:05:18.348035       1 server_linux.go:170] "Using iptables Proxier"
I0603 14:05:18.445839       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0603 14:05:18.543310       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0603 14:05:18.638191       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0603 14:05:18.639996       1 server.go:497] "Version info" version="v1.32.0"
I0603 14:05:18.640066       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0603 14:05:18.737930       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0603 14:05:18.838998       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0603 14:05:18.849064       1 config.go:199] "Starting service config controller"
I0603 14:05:18.849101       1 config.go:105] "Starting endpoint slice config controller"
I0603 14:05:18.849285       1 config.go:329] "Starting node config controller"
I0603 14:05:18.849400       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0603 14:05:18.849401       1 shared_informer.go:313] Waiting for caches to sync for service config
I0603 14:05:18.849405       1 shared_informer.go:313] Waiting for caches to sync for node config
I0603 14:05:19.038614       1 shared_informer.go:320] Caches are synced for node config
I0603 14:05:19.038699       1 shared_informer.go:320] Caches are synced for service config
I0603 14:05:19.050480       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [548011b73403] <==
I0603 13:24:46.243104       1 server_linux.go:66] "Using iptables proxy"
I0603 13:24:47.340172       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0603 13:24:47.340314       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0603 13:24:47.541641       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0603 13:24:47.541827       1 server_linux.go:170] "Using iptables Proxier"
I0603 13:24:47.551964       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0603 13:24:47.598163       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0603 13:24:47.636424       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0603 13:24:47.638690       1 server.go:497] "Version info" version="v1.32.0"
I0603 13:24:47.638814       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0603 13:24:47.685321       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0603 13:24:47.718019       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0603 13:24:47.739985       1 config.go:329] "Starting node config controller"
I0603 13:24:47.740772       1 config.go:105] "Starting endpoint slice config controller"
I0603 13:24:47.740835       1 config.go:199] "Starting service config controller"
I0603 13:24:47.741263       1 shared_informer.go:313] Waiting for caches to sync for service config
I0603 13:24:47.741303       1 shared_informer.go:313] Waiting for caches to sync for node config
I0603 13:24:47.741596       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0603 13:24:47.842628       1 shared_informer.go:320] Caches are synced for service config
I0603 13:24:47.842830       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0603 13:24:47.842848       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [bf7aa8778bb7] <==
I0603 14:05:05.260521       1 serving.go:386] Generated self-signed cert in-memory
W0603 14:05:07.642151       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0603 14:05:07.642391       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0603 14:05:07.642434       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0603 14:05:07.642450       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0603 14:05:07.940905       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0603 14:05:07.940971       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0603 14:05:07.951613       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0603 14:05:07.952505       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0603 14:05:07.952857       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0603 14:05:07.952931       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0603 14:05:08.137500       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [fd68cff331bc] <==
I0603 13:24:35.783432       1 serving.go:386] Generated self-signed cert in-memory
W0603 13:24:37.239059       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0603 13:24:37.239124       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0603 13:24:37.239140       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0603 13:24:37.239149       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0603 13:24:37.345244       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0603 13:24:37.345305       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0603 13:24:37.350586       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0603 13:24:37.351043       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0603 13:24:37.351122       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0603 13:24:37.351826       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0603 13:24:37.452237       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0603 14:03:34.849032       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0603 14:03:34.849623       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0603 14:03:34.850670       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Jun 03 14:08:44 minikube kubelet[1715]: E0603 14:08:44.240878    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:08:45 minikube kubelet[1715]: E0603 14:08:45.239991    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:08:58 minikube kubelet[1715]: E0603 14:08:58.238770    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:08:59 minikube kubelet[1715]: E0603 14:08:59.235512    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:09:09 minikube kubelet[1715]: E0603 14:09:09.239038    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:09:14 minikube kubelet[1715]: E0603 14:09:14.238597    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:09:18 minikube kubelet[1715]: E0603 14:09:18.269534    1715 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 03 14:09:18 minikube kubelet[1715]: E0603 14:09:18.269813    1715 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert podName:df1b2a69-f888-4598-beca-7d03d6bc58c7 nodeName:}" failed. No retries permitted until 2025-06-03 14:11:20.269773444 +0000 UTC m=+380.387665634 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-hzrd9" (UID: "df1b2a69-f888-4598-beca-7d03d6bc58c7") : secret "ingress-nginx-admission" not found
Jun 03 14:09:20 minikube kubelet[1715]: E0603 14:09:20.237856    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:09:27 minikube kubelet[1715]: E0603 14:09:27.236580    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:09:28 minikube kubelet[1715]: E0603 14:09:28.232086    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-hzrd9" podUID="df1b2a69-f888-4598-beca-7d03d6bc58c7"
Jun 03 14:09:35 minikube kubelet[1715]: E0603 14:09:35.232947    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:09:42 minikube kubelet[1715]: E0603 14:09:42.233916    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:09:47 minikube kubelet[1715]: E0603 14:09:47.235118    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:09:56 minikube kubelet[1715]: E0603 14:09:56.231636    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:09:59 minikube kubelet[1715]: E0603 14:09:59.231491    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:10:07 minikube kubelet[1715]: E0603 14:10:07.231265    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:10:11 minikube kubelet[1715]: E0603 14:10:11.231606    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:10:22 minikube kubelet[1715]: E0603 14:10:22.231437    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:10:23 minikube kubelet[1715]: E0603 14:10:23.231870    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:10:34 minikube kubelet[1715]: E0603 14:10:34.231249    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:10:36 minikube kubelet[1715]: E0603 14:10:36.230421    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:10:47 minikube kubelet[1715]: E0603 14:10:47.228778    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:10:51 minikube kubelet[1715]: E0603 14:10:51.226375    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:00 minikube kubelet[1715]: E0603 14:11:00.397833    1715 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Jun 03 14:11:00 minikube kubelet[1715]: E0603 14:11:00.397957    1715 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Jun 03 14:11:00 minikube kubelet[1715]: E0603 14:11:00.398156    1715 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:create,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f,Command:[],Args:[create --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc --namespace=$(POD_NAMESPACE) --secret-name=ingress-nginx-admission],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mb4ss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-create-4qqlr_ingress-nginx(1f3059a5-648f-4ca8-b280-5f2bbf4191a9): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Jun 03 14:11:00 minikube kubelet[1715]: E0603 14:11:00.399568    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:11:06 minikube kubelet[1715]: E0603 14:11:06.399614    1715 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Jun 03 14:11:06 minikube kubelet[1715]: E0603 14:11:06.399798    1715 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" image="registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Jun 03 14:11:06 minikube kubelet[1715]: E0603 14:11:06.400064    1715 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:patch,Image:registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f,Command:[],Args:[patch --webhook-name=ingress-nginx-admission --namespace=$(POD_NAMESPACE) --patch-mutating=false --secret-name=ingress-nginx-admission --patch-failure-policy=Fail],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:POD_NAMESPACE,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2vml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ingress-nginx-admission-patch-2wdwd_ingress-nginx(7d695b08-e5b0-4bb0-a1c7-03e61140a231): ErrImagePull: Error response from daemon: Get \"https://registry.k8s.io/v2/\": tls: failed to verify certificate: x509: certificate signed by unknown authority" logger="UnhandledError"
Jun 03 14:11:06 minikube kubelet[1715]: E0603 14:11:06.401782    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:15 minikube kubelet[1715]: E0603 14:11:15.228128    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:11:20 minikube kubelet[1715]: E0603 14:11:20.228277    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:20 minikube kubelet[1715]: E0603 14:11:20.267652    1715 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 03 14:11:20 minikube kubelet[1715]: E0603 14:11:20.267924    1715 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert podName:df1b2a69-f888-4598-beca-7d03d6bc58c7 nodeName:}" failed. No retries permitted until 2025-06-03 14:13:22.267885083 +0000 UTC m=+502.396221301 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-hzrd9" (UID: "df1b2a69-f888-4598-beca-7d03d6bc58c7") : secret "ingress-nginx-admission" not found
Jun 03 14:11:30 minikube kubelet[1715]: E0603 14:11:30.226547    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:11:31 minikube kubelet[1715]: E0603 14:11:31.225580    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:43 minikube kubelet[1715]: E0603 14:11:43.223469    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:44 minikube kubelet[1715]: E0603 14:11:44.220826    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-hzrd9" podUID="df1b2a69-f888-4598-beca-7d03d6bc58c7"
Jun 03 14:11:45 minikube kubelet[1715]: E0603 14:11:45.224729    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:11:55 minikube kubelet[1715]: E0603 14:11:55.219297    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:11:59 minikube kubelet[1715]: E0603 14:11:59.219424    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:12:08 minikube kubelet[1715]: E0603 14:12:08.222637    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:12:11 minikube kubelet[1715]: E0603 14:12:11.220638    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:12:19 minikube kubelet[1715]: E0603 14:12:19.222171    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:12:23 minikube kubelet[1715]: E0603 14:12:23.221440    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:12:30 minikube kubelet[1715]: E0603 14:12:30.217807    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:12:35 minikube kubelet[1715]: E0603 14:12:35.218427    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:12:43 minikube kubelet[1715]: E0603 14:12:43.220879    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:12:49 minikube kubelet[1715]: E0603 14:12:49.225052    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:12:57 minikube kubelet[1715]: E0603 14:12:57.217357    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:13:01 minikube kubelet[1715]: E0603 14:13:01.214361    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:13:09 minikube kubelet[1715]: E0603 14:13:09.216977    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:13:15 minikube kubelet[1715]: E0603 14:13:15.217049    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:13:21 minikube kubelet[1715]: E0603 14:13:21.211767    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"
Jun 03 14:13:22 minikube kubelet[1715]: E0603 14:13:22.279932    1715 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jun 03 14:13:22 minikube kubelet[1715]: E0603 14:13:22.280029    1715 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert podName:df1b2a69-f888-4598-beca-7d03d6bc58c7 nodeName:}" failed. No retries permitted until 2025-06-03 14:15:24.280014025 +0000 UTC m=+624.421177304 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/df1b2a69-f888-4598-beca-7d03d6bc58c7-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-hzrd9" (UID: "df1b2a69-f888-4598-beca-7d03d6bc58c7") : secret "ingress-nginx-admission" not found
Jun 03 14:13:29 minikube kubelet[1715]: E0603 14:13:29.215284    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"create\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-create-4qqlr" podUID="1f3059a5-648f-4ca8-b280-5f2bbf4191a9"
Jun 03 14:13:36 minikube kubelet[1715]: E0603 14:13:36.212716    1715 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with ImagePullBackOff: \"Back-off pulling image \\\"registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f\\\": ErrImagePull: Error response from daemon: Get \\\"https://registry.k8s.io/v2/\\\": tls: failed to verify certificate: x509: certificate signed by unknown authority\"" pod="ingress-nginx/ingress-nginx-admission-patch-2wdwd" podUID="7d695b08-e5b0-4bb0-a1c7-03e61140a231"


==> storage-provisioner [2626b53ab130] <==
I0603 14:05:15.850519       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0603 14:05:37.088829       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [da1eeea7dde1] <==
I0603 14:05:52.453316       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0603 14:05:52.464742       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0603 14:05:52.464911       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0603 14:06:09.870980       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0603 14:06:09.871320       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_7b25044f-fcab-4c08-85bc-8ea739346307!
I0603 14:06:09.871371       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2a726b6b-c6f0-4901-b8f7-8ab6c42070f9", APIVersion:"v1", ResourceVersion:"15837", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_7b25044f-fcab-4c08-85bc-8ea739346307 became leader
I0603 14:06:09.973915       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_7b25044f-fcab-4c08-85bc-8ea739346307!

